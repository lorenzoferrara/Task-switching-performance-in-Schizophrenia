---
title: "Actividad #1 by Viviana Giorgi"
output: html_document
---

### Importing dataset on breast cancer patients
```{r}
datos <- read.csv(file="datos_p1.csv", sep=";", dec=",", header=T)
```
```{r}
summary(datos)
```
Our goal now is to convert the variables that correspond to "character" to the type "factor" in order to do some statistical inference on them.
I am going to convert the PCR variable and save it in a new column, since for the logistic regression we will need the variable to be numerical.

```{r}
datos$REst <- as.factor(datos$REst)
datos$RPro <- as.factor(datos$RPro)
datos$Her2 <- as.factor(datos$Her2)
datos$NodAfec <- as.factor(datos$NodAfec)
datos$Estadio <- as.factor(datos$Estadio)
datos$Grado <- as.factor(datos$Grado)
datos$Fenotipo <- as.factor(datos$Fenotipo)
datos$PCR_fact <- as.factor(datos$PCR)
```
Looking at the summary again after the conversion:
```{r}
summary(datos)
```
Now, the variables that were defined as "character" types, are divided in categories, which is the format that we want.

Alternatively, I could have added another part of code to the read.csv command, like so:
datos <- read.csv(file="datos_p1.csv", sep=";", dec=",", header=T, stringsAsFactors = T))


### Descriptive statistics: overview of some variables
Looking at graphs of some variables. In order to do so, I can create a function called "AnalisisUnivariante" that does both the barplot and the pie chart of a single variable. I am then going to execute the function on some variables in our dataset.

```{r}
AnalisisUnivariante <- function(variable){
  barplot(table(variable))
  pie(table(variable))
}
```

```{r}
AnalisisUnivariante(datos$Estadio)
AnalisisUnivariante(datos$Fenotipo)
AnalisisUnivariante(datos$Grado)
```

Mean, variance, and standard deviation of numerical variable Edad, which is the only numerical one we have.
```{r}
mean(datos$Edad) 
var(datos$Edad) 
sd(datos$Edad)
```

### Datos perdidos analysis: how many of our values are NA?
```{r}
a <- is.na(datos$Edad)
sum(a)
a <- is.na(datos$REst)
sum(a)
a <- is.na(datos$RPro)
sum(a)
a <- is.na(datos$Her2)
sum(a)
a <- is.na(datos$Estadio)
sum(a)
a <- is.na(datos$NodAfec)
sum(a)
a <- is.na(datos$Grado)
sum(a)
a <- is.na(datos$Fenotipo)
sum(a)
a <- is.na(datos$PCR)
sum(a)
```
Now we can remove the NA values with command na.omit, and then compare the behavior of the varaiables before and after the removal.
For example, we can use Grado since it's the variable with the most NA values (37).

```{r}
AnalisisUnivariante(datos$Grado)
AnalisisUnivariante(na.omit(datos$Grado))

```

As we can see, the graphs are exactly the same. Indeed, R just ignores the NA values in the first case, meaning that once we remove them, R does not notice the difference.
We could also observe the change in the mean of the only numerical variable Edad, but this variable has 0 NA values, meaning no alterations can be made.


**Turning Edad into a categorical variable**
```{r}
datos$Edad_cat <- cut(datos$Edad, breaks=c(20,40,60,max(datos$Edad)))
barplot(table(datos$Edad_cat))
```

VS histogram of Edad as a numerical variable
```{r}
hist(datos$Edad, xlab= "Edad", main= "Histogram of Edad as a numerical variable")
```

### BIVARIATE ANALYSIS: studying relationship between PCR and other variables
I am going to create a function called "BivariateAnalysis", and then use it on some categorical variables to compare them we the variable we are studying, which is PCR. 
We want to observe the behavior of PCR compared to other variables of our dataset. PCR is a binary value that indicates if the tumor of the patient observed has metastasis (=1) or not (=0). Of course the 'positive' value is no metastasis, so our goal is to try to find what values of other variables are more likely to have PCR=1, in order to avoid them, or pay more attention to them in an hospital context.

```{r}
BivariateAnalysis <- function(var1,var2){
  barplot(table(var1, var2), cex.names=0.75, las = 2, legend.text= T)
}

```

**PCR and Fenotipo**
```{r}
BivariateAnalysis(datos$PCR,datos$Fenotipo)
```

Here we can see a clear connection among the presence of metastasis (PCR=1) and the Fenotipo = Basal.

**PCR and Estadio**
```{r}
BivariateAnalysis(datos$PCR,datos$Estadio)
```

Here the connection is between PCR=1 and Estadio= T2 or Estadio=T3

**PCR and Grado**
```{r}
BivariateAnalysis(datos$PCR,datos$Grado)
```

From this graph we can see that PCR=1 is more frequent when Grado=3.

Now, studying the table of contingency: this tells us how many times a certain value of Grado appears when PCR=0 or PCR=1. The results are, of course, the same as with the barplot performed above. 
```{r}
PCR.gr.tab <- xtabs(~ PCR + Grado, data = datos)
PCR.gr.tab
```

We can see that some of the values are <5. In this case the recommended test between the two variables is the fisher test, otherwise it would be the chi squared test.

```{r}
fisher.test(xtabs(~ PCR + Grado, data = datos))
```

For 2x2 tables, H0 of conditional independence is equivalent to the hypothesis that the odds ratio equals one.
The p-value is very low (limit value= 5%), meaning that the null hypothesis of independence can be discarded, and according to the Fisher exact test, the two variables are dependent.


Trying the xtabs command with two different variables: here the sample is much larger (no values in the contingency table is <5), meaning that we can use the Chi squared test.
```{r}
PCR.nod.tab <- xtabs(~ PCR + NodAfec, data = datos)
PCR.nod.tab
```

```{r}
res <- chisq.test(PCR.nod.tab)
res$p.value
```

Here the p-value is ~70%, meaning that no correlation can be proven between variables PCR and NodAfec.



**Boxplot to study two variables - one categorical and one numerical: Edad and PCR**
```{r}
boxplot(datos$Edad ~ datos$PCR_fact, ylab= "Edad", xlab="PCR")

```

From the graph above, unfortunately not much can be inferred. Distribution of the two values of PCR is pretty similar, except for older patients (Edad>65) where PCR=1 is less frequent.

We can use some interesting tests in order to prove this more concretely. First, we have to check if these variables follow a normal distribution, or our tests will not make any sense.
```{r}
hist(datos$Edad, xlab="Edad")
shapiro.test(datos$Edad)
```

P-value is 0.07%, meaning that we have to discard the null hypothesis. The numerical variable Edad does not follow a normal distribution. This means that we can't perform a t.test for the analysis of two groups, but we have to perform the Wilcoxon rank sum test (PCR only contains two groups; if it contained more than 2, we would have to use the Kruskal-Wallis rank sum test)

```{r}
wilcox.test(Edad ~ PCR_fact, data = datos)
```

P-value is high, meaning we don't discard the null hypothesis.

## Generic function for Bivariate Analysis
After working with specific variables, here I will write the code for a generic function which analizes two different generic variables that the user decides, and performs the best test according to their type and distribution.

Remark: this function assumes we only have factor or numerical variables.


```{r}
GenericFunction <- function(var1,var2){
  if ((is.factor(var1)=="TRUE") && (is.factor(var2)=="TRUE")){
    # both variables are factors
    barplot(table(var1, var2), cex.names=0.75, las = 2, legend.text= T)
    tab <- xtabs(~ var1 + var2, data = datos)
    tab
    
    #checking if there are values smaller than 5 in tab
    mark<-FALSE
    for (i in 1:nrow(tab)){
      for (j in 1:ncol(tab)){
        if (tab[i,j]<5){
          mark<-TRUE}
      }
    }
    if (mark=="FALSE"){ 
      # i can perform the chisquared test
      res <- chisq.test(tab)
      print(res$method)
      print(res$p.value)
    }
    else{
      #i have to perform the fisher test
      res <- fisher.test(xtabs(~ var1 + var2, data = datos), simulate.p.value = TRUE)
      print(res$method)
      print(res$p.value)
    }
  }
  
  if ((is.factor(var1)=="TRUE") && (is.factor(var2)=="FALSE")){
    #the first is a factor and the second is numerical
    boxplot(var2 ~ var1)
    
    #check if the numerical variables follows a normal distribution
    hist(var2)
    
    print(shapiro.test(var2)$method)
    print(shapiro.test(var2)$p.value)
   
    
    if (shapiro.test(var2)$p.value > 0.05){
      # var2 follows a normal distribution
      if (length(levels(var1))>2){
        # more than 2 groups for the categorical variable
        summary(aov(var2 ~ var1, data = datos))
      }
      else{
        # just two groups
        print(t.test(var2 ~ var1, data = datos)$method)
        print(t.test(var2 ~ var1, data = datos)$p.value)
        
      }
    }
    
    if (shapiro.test(var2)$p.value < 0.05){
      # variable does not follow a normal distribution
      if (length(levels(var1))>2){
        # more than 2 groups for the categorical variable
        print(kruskal.test(var2 ~ var1, data = datos)$method)
        print(kruskal.test(var2 ~ var1, data = datos)$p.value)
        
      }
      else{
        # just two groups
        print(wilcox.test(var2 ~ var1, data = datos)$method)
        print(wilcox.test(var2 ~ var1, data = datos)$p.value)
      }
    }
      
  }
  
 if ((is.factor(var1)=="FALSE") && (is.factor(var2)=="TRUE")){
    #the second is a factor and the first is numerical
    boxplot(var1 ~ var2)
    
    #check if the numerical variables follows a normal distribution
    hist(var1)
    
    print(shapiro.test(var1)$method)
    print(shapiro.test(var1)$p.value)
   
    
    if (shapiro.test(var1)$p.value > 0.05){
      # var2 follows a normal distribution
      if (length(levels(var2))>2){
        # more than 2 groups for the categorical variable
        summary(aov(var1 ~ var2, data = datos))
      }
      else{
        # just two groups
        print(t.test(var1 ~ var2, data = datos)$method)
        print(t.test(var1 ~ var2, data = datos)$p.value)
        
      }
    }
    
    if (shapiro.test(var1)$p.value < 0.05){
      # variable does not follow a normal distribution
      if (length(levels(var2))>2){
        # more than 2 groups for the categorical variable
        print(kruskal.test(var1 ~ var2, data = datos)$method)
        print(kruskal.test(var1 ~ var2, data = datos)$p.value)
        
      }
      else{
        # just two groups
        print(wilcox.test(var1 ~ var2, data = datos)$method)
        print(wilcox.test(var1 ~ var2, data = datos)$p.value)
      }
    }
      
  }
  
  if ((is.factor(var1)=="FALSE") && (is.factor(var2)=="FALSE")){
    #both are numerical
    plot(var1,var2)
    if ((shapiro.test(var1)$p.value > 0.05) & (shapiro.test(var2)$p.value > 0.05)){
      #both variables are normal
      print(cor.test(var1, var2, alternative="two.sided", method="pearson")$method)
      print(cor.test(var1, var2, alternative="two.sided", method="pearson")$p.value)
    }
    else{
      print(cor.test(var1, var2, alternative="two.sided", method="spearman")$method)
      print(cor.test(var1, var2, alternative="two.sided", method="spearman")$p.value)
    }
  }
  
}
```

Let's try the function with a couple of examples:
```{r}
GenericFunction(datos$PCR_fact,datos$Edad)
GenericFunction(datos$Grado,datos$Fenotipo)
GenericFunction(datos$PCR_fact, datos$Estadio)
```


### MULTIVARIATE ANALYSIS: Logistic Regression

In the logistic regression model, "Intercept"=beta0, the intercept of the line with the y axis, and "Edad"=beta1, the coefficient of the line that expresses correlation between the two variables.

We want to try and find a correlation between PCR=1 and Edad (our numerical variable)

```{r}
lr.fit1 <- glm(PCR ~ Edad, data = datos, family = binomial("logit"))
summary(lr.fit1)
newdata <- data.frame(Edad=seq(min(datos$Edad), max(datos$Edad),len=500))
newdata$out = predict(lr.fit1, newdata, type="response")

plot(datos$Edad, datos$PCR, xlab="Edad", ylab="PCR")
lines(out ~ Edad, newdata, lwd=2, col="blue")
```

As we can see in the summary, p-value for Edad is high, meaning it is not very significative for the linear regression model. This is also specified by R, which puts *** next to the variables that are significative.
In any case, we can see that "Edad"=-0.01800 in our summary of the model, meaning that the two variables are "correlated" (even if as seen, they are not) in a negative way. This means that the smaller the age of the patient, the more likely is PCR to take the value 1.



### Stepwise Regression

Now we can try to perform the technique of Stepwise Regression. Thanks to the library MASS, R is able to find the most fitting variables that are dependent on one another, and create a Regression model with them.

First, I'm removing PCR_fact and Edad_cat since they won't be needed in the regression.

```{r}
library(MASS)
```


```{r}
datos <- subset (datos, select = -c(Edad_cat,PCR_fact))
summary(datos)
```

I have to to eliminate all the NA values: as we said before, our bivariate analysis part did not care about NAs and just ignored them; now we have to eliminate them, or the code will not compile.
As we have seen on the part of Analisis de Datos Perdidos, only 74 NA values are present overall: this means that when removing all of them, we are removing only around 74/508=14% of the dataset, which is not great, but is something we can't avoid.

```{r}
datos <- na.omit(datos)
```

Let's start with Backwards regression; the first things to do is use glm and insert all the variables we have.
REMARK: Muestra is considered a variable (instead, it's just the code for each patient) so I had to remove it from the model.

```{r}
lr.fit1 <- glm(PCR ~ . - Muestra, data = datos, family = binomial("logit"))
summary(lr.fit1)
```

As we can see from the summary, the only important variables (as seen from the p-value) are FenotipoHer2 and FenotipoLumA, the others are not very relevant to the model.
We also have to take into account the AIC value: it will be important to notice how it changes after discarding some of the variables.

```{r}
lr.fit.back <- stepAIC(lr.fit1, direction = "backward")
```

After the use of the backwards regression, our model is
PCR ~ Estadio + Grado + Fenotipo with AIC= 385.6 (we started from 401)

```{r}
summary(lr.fit.back)
```

The summary shows us the p-value for the variables that were included in the model, and the ones denoted by *** are the ones that denote an high correlation to PCR.
What does this mean? This means that a very important factor in a patient is the value that variable Fenotipo assumes. Since the coefficient is negative, PCR=1 is least likely to appear if Fenotipo=LumA (which has the biggest value, -2.0827)
Let's put this to the test. We can have our model predict the probability of PCR=1 starting from the data of a patient.
I will put some random numbers: what is the probability of PCR=1 if Fenotipo= Normal or Fenotipo=LumA?

```{r}
predict(lr.fit.back, newdata = data.frame(Grado ="1", Estadio="T2", Fenotipo="Normal"), type = "response")
```

```{r}
predict(lr.fit.back, newdata = data.frame(Grado ="1", Estadio="T2", Fenotipo="LumA"), type = "response")
```

As we have said, in the last case the probability is much smaller.

Now we want to try the forward regression: we start with an empty model and add variables to have our criteria met in the best way possible.

```{r}
lr.fit2 <- glm(PCR~ 1, data = datos, family = binomial("logit"))
stepAIC(lr.fit2, direction = "forward", scope = list(upper = lr.fit1, lower = lr.fit2))
```

As could be imagined, the end model is the same as before.

###-----------------------------------------------------------------------

#### Computation of the accuracy for the different models



```{r}
#AccuracyComp <- function(model_name, total){
# the function also gets as input the total observations of the model
  
#model.fit <- predict(...)

  

  
#check <- a==b
#number <- sum(check)
  
  
#acc= number/total
#return (list(acc, tabla)
#}

```

















